{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT on long texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we explore different approach to overcome one of the main limitation of BERT (which stands for Bidirectional Encoder Representations from Transformers), the ability to process long document. In fact BERT can only be applied on text that have less than 512 token after tokenization with the Bert Tokenizer.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "We will implement [this paper, which introduce a new method to deal with Long Documents : RoBERT (Recurrence over BERT)](https://arxiv.org/abs/1910.10781).\n",
    "\n",
    "\n",
    "\n",
    "We will also implement [this paper, which introduce diferents methods to deal with Long Documents and BERT](https://arxiv.org/abs/1905.05583) to see if the RoBERT paper bring some significative improvement on the classification of Long Texts with BERT.\n",
    "\n",
    "This paper introduce 2 main approaches, the **Truncation methods** and the **Hierarchical methods** :\n",
    " * Truncation methods\n",
    "   * head-only\n",
    "   * tail-only\n",
    "   * head+tail\n",
    " * Hierarchical methods\n",
    "   * mean pooling\n",
    "   * max pooling\n",
    " \n",
    "The Truncation methods applies to the input of the BERT model (the Tokens), while the Hierarchical methods applies to the ouputs of the Bert model (the embbeding), we will go into more detail in the respective parts\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "The goal of the original RoBERT article was to solve the following problem: BERT has a fixed input token count; how can we use its power on long texts. This notebook implements the same approach using HuggingFace's `transformers` library and `pytorch`.\n",
    "\n",
    "The dataset used is the *US Consumer Finance Complaints* available on [Kaggle](https://www.kaggle.com/cfpb/us-consumer-finance-complaints).\n",
    "\n",
    "Basically, the article goes as follows:\n",
    "1. Read the data and do some basic preprocessing\n",
    "2. Break the documents into smaller segments with a number of tokens that can be handled by BERT\n",
    "3. Fine-tune BERT on those segments using a classification head\n",
    "4. Combine the segments of each document by using an LSTM. The fixed output size of the LSTM can be used by a single fully connected layer for the final classification.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "For code clarity, we separated out some parts of the code into python scripts that are fully commented. They will be referenced throughout the notebook.\n",
    "\n",
    "Here is a graph that represents the differents Interaction between the differents Classes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/Class_Interactions.png](img/Class_Interactions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, BertTokenizer, RobertaModel, BertModel, AdamW# get_linear_schedule_with_warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "\n",
    "from utils import *\n",
    "from Custom_Dataset_Class import ConsumerComplaintsDataset1\n",
    "from Bert_Classification import Bert_Classification_Model\n",
    "from RoBERT import RoBERT_Model\n",
    "\n",
    "from BERT_Hierarchical import BERT_Hierarchical_Model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:1\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this work was retrieved from kaggle as said in the paper, these are consumer complaints about financial products and services which are sent by the CFPB (Consumer FinancialProtection  Bureau)  to  the  company  for  answer.\n",
    "\n",
    "The  dataset  consists  of  555957  rows  and  18columns. \n",
    "\n",
    "As our model attempts to predict which product the complaint is about, we only used the consumer-complaint-narrative and product columns.\n",
    "* comsumer-complaint-narrative:  contains the consumer complaint in text format.\n",
    "* product:  label of the product concerned by the complaint\\\n",
    "\n",
    "The final dataset used in this work consists of 555957 rows and 2 columns (one column for the texts and the other for the labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 7,753\n",
      "\n",
      "Number of training sentences with complain narrative not null: 7,753\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>labels</th>\n",
       "      <th>generated</th>\n",
       "      <th>augmented_sentences</th>\n",
       "      <th>preceding_sentences</th>\n",
       "      <th>following_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>I talked about in some detail the security val...</td>\n",
       "      <td>0</td>\n",
       "      <td>Preceding sentences: \\n\\nDuring this quarter, ...</td>\n",
       "      <td>I talked about in some detail the security val...</td>\n",
       "      <td>During this quarter, we had some exciting upda...</td>\n",
       "      <td>Our team has been working relentlessly to impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7336</th>\n",
       "      <td>And then that’s all just on Azure.</td>\n",
       "      <td>0</td>\n",
       "      <td>Preceding sentence: We are seeing tremendous g...</td>\n",
       "      <td>And then that’s all just on Azure.</td>\n",
       "      <td>We are seeing tremendous growth in our cloud c...</td>\n",
       "      <td>Additionally, we are continuing to invest in e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4184</th>\n",
       "      <td>We're excited about the number of customers th...</td>\n",
       "      <td>1</td>\n",
       "      <td>Preceding sentence: Our latest product release...</td>\n",
       "      <td>We're excited about the number of customers th...</td>\n",
       "      <td>Our latest product release has been generating...</td>\n",
       "      <td>As a result, we are seeing a significant uptic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6439</th>\n",
       "      <td>One of the things I would point out is that fr...</td>\n",
       "      <td>0</td>\n",
       "      <td>Preceding: \"Now, turning to our revenue outloo...</td>\n",
       "      <td>One of the things I would point out is that fr...</td>\n",
       "      <td>Now, turning to our revenue outlook for the ne...</td>\n",
       "      <td>However, despite these challenges, we remain c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>I’m sorry I skipped over that gross margin que...</td>\n",
       "      <td>0</td>\n",
       "      <td>Preceding sentences: \"We've received a lot of ...</td>\n",
       "      <td>I’m sorry I skipped over that gross margin que...</td>\n",
       "      <td>We've received a lot of questions today regard...</td>\n",
       "      <td>We attribute the increase in gross margins to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2737</th>\n",
       "      <td>And on a sequential basis, when we look into t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Preceding sentence: Our international operatio...</td>\n",
       "      <td>And on a sequential basis, when we look into t...</td>\n",
       "      <td>Our international operations have been a signi...</td>\n",
       "      <td>Despite the currency headwinds, we remain opti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>And so the other things that you named, whethe...</td>\n",
       "      <td>1</td>\n",
       "      <td>Preceding: \"We're now going to open the call u...</td>\n",
       "      <td>And so the other things that you named, whethe...</td>\n",
       "      <td>We're now going to open the call up to questio...</td>\n",
       "      <td>Looking ahead, we're excited to continue expan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>And so the inherent volatility that you are ta...</td>\n",
       "      <td>1</td>\n",
       "      <td>Preceding sentence:\\nWe've noticed that there'...</td>\n",
       "      <td>And so the inherent volatility that you are ta...</td>\n",
       "      <td>We've noticed that there's been some concern a...</td>\n",
       "      <td>That being said, we're confident in our abilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>And the link to basically do the messaging is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Preceding sentence: We're now going to open up...</td>\n",
       "      <td>And the link to basically do the messaging is ...</td>\n",
       "      <td>We're now going to open up the call for questi...</td>\n",
       "      <td>So, we're actively working on improving that i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7055</th>\n",
       "      <td>And so, in order to have that local compute in...</td>\n",
       "      <td>0</td>\n",
       "      <td>Preceding sentences: \"We've been seeing a lot ...</td>\n",
       "      <td>And so, in order to have that local compute in...</td>\n",
       "      <td>We've been seeing a lot of interest in our Azu...</td>\n",
       "      <td>We're continuing to invest in Azure Stack to m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentences  labels  \\\n",
       "1250  I talked about in some detail the security val...       0   \n",
       "7336                 And then that’s all just on Azure.       0   \n",
       "4184  We're excited about the number of customers th...       1   \n",
       "6439  One of the things I would point out is that fr...       0   \n",
       "1789  I’m sorry I skipped over that gross margin que...       0   \n",
       "2737  And on a sequential basis, when we look into t...       1   \n",
       "2287  And so the other things that you named, whethe...       1   \n",
       "1565  And so the inherent volatility that you are ta...       1   \n",
       "2047  And the link to basically do the messaging is ...       1   \n",
       "7055  And so, in order to have that local compute in...       0   \n",
       "\n",
       "                                              generated  \\\n",
       "1250  Preceding sentences: \\n\\nDuring this quarter, ...   \n",
       "7336  Preceding sentence: We are seeing tremendous g...   \n",
       "4184  Preceding sentence: Our latest product release...   \n",
       "6439  Preceding: \"Now, turning to our revenue outloo...   \n",
       "1789  Preceding sentences: \"We've received a lot of ...   \n",
       "2737  Preceding sentence: Our international operatio...   \n",
       "2287  Preceding: \"We're now going to open the call u...   \n",
       "1565  Preceding sentence:\\nWe've noticed that there'...   \n",
       "2047  Preceding sentence: We're now going to open up...   \n",
       "7055  Preceding sentences: \"We've been seeing a lot ...   \n",
       "\n",
       "                                    augmented_sentences  \\\n",
       "1250  I talked about in some detail the security val...   \n",
       "7336                 And then that’s all just on Azure.   \n",
       "4184  We're excited about the number of customers th...   \n",
       "6439  One of the things I would point out is that fr...   \n",
       "1789  I’m sorry I skipped over that gross margin que...   \n",
       "2737  And on a sequential basis, when we look into t...   \n",
       "2287  And so the other things that you named, whethe...   \n",
       "1565  And so the inherent volatility that you are ta...   \n",
       "2047  And the link to basically do the messaging is ...   \n",
       "7055  And so, in order to have that local compute in...   \n",
       "\n",
       "                                    preceding_sentences  \\\n",
       "1250  During this quarter, we had some exciting upda...   \n",
       "7336  We are seeing tremendous growth in our cloud c...   \n",
       "4184  Our latest product release has been generating...   \n",
       "6439  Now, turning to our revenue outlook for the ne...   \n",
       "1789  We've received a lot of questions today regard...   \n",
       "2737  Our international operations have been a signi...   \n",
       "2287  We're now going to open the call up to questio...   \n",
       "1565  We've noticed that there's been some concern a...   \n",
       "2047  We're now going to open up the call for questi...   \n",
       "7055  We've been seeing a lot of interest in our Azu...   \n",
       "\n",
       "                                    following_sentences  \n",
       "1250  Our team has been working relentlessly to impr...  \n",
       "7336  Additionally, we are continuing to invest in e...  \n",
       "4184  As a result, we are seeing a significant uptic...  \n",
       "6439  However, despite these challenges, we remain c...  \n",
       "1789  We attribute the increase in gross margins to ...  \n",
       "2737  Despite the currency headwinds, we remain opti...  \n",
       "2287  Looking ahead, we're excited to continue expan...  \n",
       "1565  That being said, we're confident in our abilit...  \n",
       "2047  So, we're actively working on improving that i...  \n",
       "7055  We're continuing to invest in Azure Stack to m...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FinArg\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df=pd.read_json(\"./augmented_data/gpt3_5_context_extracted.json\")\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "train_raw = df\n",
    "print('Number of training sentences with complain narrative not null: {:,}\\n'.format(train_raw.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "# train_raw.columns = ['text', 'label']\n",
    "train_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 555,957\n",
      "\n",
      "Number of training sentences with complain narrative not null: 66,806\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_received</th>\n",
       "      <th>product</th>\n",
       "      <th>sub_product</th>\n",
       "      <th>issue</th>\n",
       "      <th>sub_issue</th>\n",
       "      <th>consumer_complaint_narrative</th>\n",
       "      <th>company_public_response</th>\n",
       "      <th>company</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>tags</th>\n",
       "      <th>consumer_consent_provided</th>\n",
       "      <th>submitted_via</th>\n",
       "      <th>date_sent_to_company</th>\n",
       "      <th>company_response_to_consumer</th>\n",
       "      <th>timely_response</th>\n",
       "      <th>consumer_disputed?</th>\n",
       "      <th>complaint_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299865</th>\n",
       "      <td>09/22/2015</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>APR or interest rate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>On my Care Credit account I had some promotion...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Synchrony Financial</td>\n",
       "      <td>NY</td>\n",
       "      <td>132XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>09/22/2015</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1574545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514169</th>\n",
       "      <td>11/05/2015</td>\n",
       "      <td>Prepaid card</td>\n",
       "      <td>Government benefit payment card</td>\n",
       "      <td>Unauthorized transactions/trans. issues</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I am one of Rush card user, I have been really...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Empowerment Ventures, LLC</td>\n",
       "      <td>LA</td>\n",
       "      <td>712XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>11/06/2015</td>\n",
       "      <td>Closed with monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1641945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509698</th>\n",
       "      <td>01/22/2016</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>Non-federal student loan</td>\n",
       "      <td>Dealing with my lender or servicer</td>\n",
       "      <td>Trouble with how payments are handled</td>\n",
       "      <td>I attended University XXXX at XXXX for two yea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Navient Solutions, Inc.</td>\n",
       "      <td>UT</td>\n",
       "      <td>841XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>01/22/2016</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1753981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254231</th>\n",
       "      <td>06/25/2015</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>FHA mortgage</td>\n",
       "      <td>Loan modification,collection,foreclosure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I am a homeowner in the state of Va. I am curr...</td>\n",
       "      <td>Company chooses not to provide a public response</td>\n",
       "      <td>U.S. Bancorp</td>\n",
       "      <td>VA</td>\n",
       "      <td>236XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>06/25/2015</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1437607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539518</th>\n",
       "      <td>04/07/2016</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Conventional adjustable mortgage (ARM)</td>\n",
       "      <td>Loan modification,collection,foreclosure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have been trying to come to a resolution for...</td>\n",
       "      <td>Company believes it acted appropriately as aut...</td>\n",
       "      <td>Select Portfolio Servicing, Inc</td>\n",
       "      <td>CA</td>\n",
       "      <td>926XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>04/07/2016</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1868877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306278</th>\n",
       "      <td>10/21/2015</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Credit monitoring or identity protection</td>\n",
       "      <td>Billing dispute</td>\n",
       "      <td>CONSUMER : XXXX XXXXCOMPLAINT : TRANSUNION AND...</td>\n",
       "      <td>Company chooses not to provide a public response</td>\n",
       "      <td>TransUnion Intermediate Holdings, Inc.</td>\n",
       "      <td>CA</td>\n",
       "      <td>913XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>10/21/2015</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1619069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509284</th>\n",
       "      <td>02/15/2016</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Conventional fixed mortgage</td>\n",
       "      <td>Loan servicing, payments, escrow account</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mortgage was approved for 2MP program in conju...</td>\n",
       "      <td>Company disputes the facts presented in the co...</td>\n",
       "      <td>West Coast Servicing, Inc.</td>\n",
       "      <td>MI</td>\n",
       "      <td>481XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>03/23/2016</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1787834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514547</th>\n",
       "      <td>02/10/2016</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Billing disputes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have XXXX accounts with Wells Fargo. The acc...</td>\n",
       "      <td>Company chooses not to provide a public response</td>\n",
       "      <td>Wells Fargo &amp; Company</td>\n",
       "      <td>DE</td>\n",
       "      <td>197XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>02/10/2016</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1779058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210058</th>\n",
       "      <td>04/07/2015</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Incorrect information on credit report</td>\n",
       "      <td>Reinserted previously deleted info</td>\n",
       "      <td>I believe that XXXX XXXX XXXX XXXX ( XXXX ) is...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Equifax</td>\n",
       "      <td>OK</td>\n",
       "      <td>735XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>04/27/2015</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1319609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227801</th>\n",
       "      <td>04/25/2015</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Incorrect information on credit report</td>\n",
       "      <td>Information is not mine</td>\n",
       "      <td>I have disputed this XXXX times. I did n't sig...</td>\n",
       "      <td>Company chooses not to provide a public response</td>\n",
       "      <td>TransUnion Intermediate Holdings, Inc.</td>\n",
       "      <td>KS</td>\n",
       "      <td>667XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>04/25/2015</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1347356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date_received           product  \\\n",
       "299865    09/22/2015       Credit card   \n",
       "514169    11/05/2015      Prepaid card   \n",
       "509698    01/22/2016      Student loan   \n",
       "254231    06/25/2015          Mortgage   \n",
       "539518    04/07/2016          Mortgage   \n",
       "306278    10/21/2015  Credit reporting   \n",
       "509284    02/15/2016          Mortgage   \n",
       "514547    02/10/2016       Credit card   \n",
       "210058    04/07/2015  Credit reporting   \n",
       "227801    04/25/2015  Credit reporting   \n",
       "\n",
       "                                   sub_product  \\\n",
       "299865                                     NaN   \n",
       "514169         Government benefit payment card   \n",
       "509698                Non-federal student loan   \n",
       "254231                            FHA mortgage   \n",
       "539518  Conventional adjustable mortgage (ARM)   \n",
       "306278                                     NaN   \n",
       "509284             Conventional fixed mortgage   \n",
       "514547                                     NaN   \n",
       "210058                                     NaN   \n",
       "227801                                     NaN   \n",
       "\n",
       "                                           issue  \\\n",
       "299865                      APR or interest rate   \n",
       "514169   Unauthorized transactions/trans. issues   \n",
       "509698        Dealing with my lender or servicer   \n",
       "254231  Loan modification,collection,foreclosure   \n",
       "539518  Loan modification,collection,foreclosure   \n",
       "306278  Credit monitoring or identity protection   \n",
       "509284  Loan servicing, payments, escrow account   \n",
       "514547                          Billing disputes   \n",
       "210058    Incorrect information on credit report   \n",
       "227801    Incorrect information on credit report   \n",
       "\n",
       "                                    sub_issue  \\\n",
       "299865                                    NaN   \n",
       "514169                                    NaN   \n",
       "509698  Trouble with how payments are handled   \n",
       "254231                                    NaN   \n",
       "539518                                    NaN   \n",
       "306278                        Billing dispute   \n",
       "509284                                    NaN   \n",
       "514547                                    NaN   \n",
       "210058     Reinserted previously deleted info   \n",
       "227801                Information is not mine   \n",
       "\n",
       "                             consumer_complaint_narrative  \\\n",
       "299865  On my Care Credit account I had some promotion...   \n",
       "514169  I am one of Rush card user, I have been really...   \n",
       "509698  I attended University XXXX at XXXX for two yea...   \n",
       "254231  I am a homeowner in the state of Va. I am curr...   \n",
       "539518  I have been trying to come to a resolution for...   \n",
       "306278  CONSUMER : XXXX XXXXCOMPLAINT : TRANSUNION AND...   \n",
       "509284  Mortgage was approved for 2MP program in conju...   \n",
       "514547  I have XXXX accounts with Wells Fargo. The acc...   \n",
       "210058  I believe that XXXX XXXX XXXX XXXX ( XXXX ) is...   \n",
       "227801  I have disputed this XXXX times. I did n't sig...   \n",
       "\n",
       "                                  company_public_response  \\\n",
       "299865                                                NaN   \n",
       "514169                                                NaN   \n",
       "509698                                                NaN   \n",
       "254231   Company chooses not to provide a public response   \n",
       "539518  Company believes it acted appropriately as aut...   \n",
       "306278   Company chooses not to provide a public response   \n",
       "509284  Company disputes the facts presented in the co...   \n",
       "514547   Company chooses not to provide a public response   \n",
       "210058                                                NaN   \n",
       "227801   Company chooses not to provide a public response   \n",
       "\n",
       "                                       company state zipcode tags  \\\n",
       "299865                     Synchrony Financial    NY   132XX  NaN   \n",
       "514169               Empowerment Ventures, LLC    LA   712XX  NaN   \n",
       "509698                 Navient Solutions, Inc.    UT   841XX  NaN   \n",
       "254231                            U.S. Bancorp    VA   236XX  NaN   \n",
       "539518         Select Portfolio Servicing, Inc    CA   926XX  NaN   \n",
       "306278  TransUnion Intermediate Holdings, Inc.    CA   913XX  NaN   \n",
       "509284              West Coast Servicing, Inc.    MI   481XX  NaN   \n",
       "514547                   Wells Fargo & Company    DE   197XX  NaN   \n",
       "210058                                 Equifax    OK   735XX  NaN   \n",
       "227801  TransUnion Intermediate Holdings, Inc.    KS   667XX  NaN   \n",
       "\n",
       "       consumer_consent_provided submitted_via date_sent_to_company  \\\n",
       "299865          Consent provided           Web           09/22/2015   \n",
       "514169          Consent provided           Web           11/06/2015   \n",
       "509698          Consent provided           Web           01/22/2016   \n",
       "254231          Consent provided           Web           06/25/2015   \n",
       "539518          Consent provided           Web           04/07/2016   \n",
       "306278          Consent provided           Web           10/21/2015   \n",
       "509284          Consent provided           Web           03/23/2016   \n",
       "514547          Consent provided           Web           02/10/2016   \n",
       "210058          Consent provided           Web           04/27/2015   \n",
       "227801          Consent provided           Web           04/25/2015   \n",
       "\n",
       "       company_response_to_consumer timely_response consumer_disputed?  \\\n",
       "299865      Closed with explanation             Yes                 No   \n",
       "514169  Closed with monetary relief             Yes                 No   \n",
       "509698      Closed with explanation             Yes                 No   \n",
       "254231      Closed with explanation             Yes                 No   \n",
       "539518      Closed with explanation             Yes                Yes   \n",
       "306278      Closed with explanation             Yes                Yes   \n",
       "509284      Closed with explanation             Yes                 No   \n",
       "514547      Closed with explanation             Yes                Yes   \n",
       "210058      Closed with explanation             Yes                Yes   \n",
       "227801      Closed with explanation             Yes                 No   \n",
       "\n",
       "        complaint_id  \n",
       "299865       1574545  \n",
       "514169       1641945  \n",
       "509698       1753981  \n",
       "254231       1437607  \n",
       "539518       1868877  \n",
       "306278       1619069  \n",
       "509284       1787834  \n",
       "514547       1779058  \n",
       "210058       1319609  \n",
       "227801       1347356  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "df=pd.read_csv(\"./us-consumer-finance-complaints/consumer_complaints.csv\")\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "train_raw = df[df.consumer_complaint_narrative.notnull()]\n",
    "print('Number of training sentences with complain narrative not null: {:,}\\n'.format(train_raw.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "train_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': \"nombre d'ocurence par nombre de mots dans un commentaire\"}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_raw.consumer_complaint_narrative.apply(lambda x: len(x.split()) if len(x.split())<800 else 800).plot(kind='hist', title=\"nombre d'ocurence par nombre de mots dans un commentaire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_id</th>\n",
       "      <th>len_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.680600e+04</td>\n",
       "      <td>66806.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.571665e+06</td>\n",
       "      <td>190.644014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.545692e+05</td>\n",
       "      <td>166.830597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.290181e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.443264e+06</td>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.569485e+06</td>\n",
       "      <td>136.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.702750e+06</td>\n",
       "      <td>254.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.888608e+06</td>\n",
       "      <td>1284.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       complaint_id       len_txt\n",
       "count  6.680600e+04  66806.000000\n",
       "mean   1.571665e+06    190.644014\n",
       "std    1.545692e+05    166.830597\n",
       "min    1.290181e+06      1.000000\n",
       "25%    1.443264e+06     71.000000\n",
       "50%    1.569485e+06    136.000000\n",
       "75%    1.702750e+06    254.000000\n",
       "max    1.888608e+06   1284.000000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw['len_txt'] =train_raw.consumer_complaint_narrative.apply(lambda x: len(x.split()))\n",
    "train_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select only the row with number of words greater than 250:\n",
    "train_raw = train_raw[train_raw.len_txt >249]\n",
    "train_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumer_complaint_narrative</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In XX/XX/XXXX my wages that I earned at my job...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XXXX was submitted XX/XX/XXXX. At the time I s...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I spoke to XXXX of green tree representatives ...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i opened XXXX Bank of America credit cards 15-...</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I applied for a loan with XXXX XXXX and had pu...</td>\n",
       "      <td>Consumer Loan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        consumer_complaint_narrative        product\n",
       "0  In XX/XX/XXXX my wages that I earned at my job...       Mortgage\n",
       "1  XXXX was submitted XX/XX/XXXX. At the time I s...       Mortgage\n",
       "2  I spoke to XXXX of green tree representatives ...       Mortgage\n",
       "3  i opened XXXX Bank of America credit cards 15-...    Credit card\n",
       "4  I applied for a loan with XXXX XXXX and had pu...  Consumer Loan"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select only the column 'consumer_complaint_narrative' and 'product'\n",
    "train_raw = train_raw[['consumer_complaint_narrative', 'product']]\n",
    "train_raw.reset_index(inplace=True, drop=True)\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumer_complaint_narrative</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In XX/XX/XXXX my wages that I earned at my job...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XXXX was submitted XX/XX/XXXX. At the time I s...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I spoke to XXXX of green tree representatives ...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i opened XXXX Bank of America credit cards 15-...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I applied for a loan with XXXX XXXX and had pu...</td>\n",
       "      <td>Consumer Loan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        consumer_complaint_narrative  \\\n",
       "0  In XX/XX/XXXX my wages that I earned at my job...   \n",
       "1  XXXX was submitted XX/XX/XXXX. At the time I s...   \n",
       "2  I spoke to XXXX of green tree representatives ...   \n",
       "3  i opened XXXX Bank of America credit cards 15-...   \n",
       "4  I applied for a loan with XXXX XXXX and had pu...   \n",
       "\n",
       "                       product  \n",
       "0                     Mortgage  \n",
       "1                     Mortgage  \n",
       "2                     Mortgage  \n",
       "3  Credit card or prepaid card  \n",
       "4                Consumer Loan  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group similar products\n",
    "train_raw.loc[train_raw['product'] == 'Credit reporting', 'product'] = 'Credit reporting, credit repair services, or other personal consumer reports'\n",
    "train_raw.loc[train_raw['product'] == 'Credit card', 'product'] = 'Credit card or prepaid card'\n",
    "train_raw.loc[train_raw['product'] == 'Prepaid card', 'product'] = 'Credit card or prepaid card'\n",
    "train_raw.loc[train_raw['product'] == 'Payday loan', 'product'] = 'Payday loan, title loan, or personal loan'\n",
    "train_raw.loc[train_raw['product'] == 'Virtual currency', 'product'] = 'Money transfer, virtual currency, or money service'\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank account or service\n",
      "Consumer Loan\n",
      "Credit card or prepaid card\n",
      "Credit reporting, credit repair services, or other personal consumer reports\n",
      "Debt collection\n",
      "Money transfers\n",
      "Mortgage\n",
      "Other financial service\n",
      "Payday loan, title loan, or personal loan\n",
      "Student loan\n"
     ]
    }
   ],
   "source": [
    "#all the different classes\n",
    "for l in np.unique(train_raw['product']):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In XX/XX/XXXX my wages that I earned at my job...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XXXX was submitted XX/XX/XXXX. At the time I s...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I spoke to XXXX of green tree representatives ...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i opened XXXX Bank of America credit cards 15-...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I applied for a loan with XXXX XXXX and had pu...</td>\n",
       "      <td>Consumer Loan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  In XX/XX/XXXX my wages that I earned at my job...   \n",
       "1  XXXX was submitted XX/XX/XXXX. At the time I s...   \n",
       "2  I spoke to XXXX of green tree representatives ...   \n",
       "3  i opened XXXX Bank of America credit cards 15-...   \n",
       "4  I applied for a loan with XXXX XXXX and had pu...   \n",
       "\n",
       "                         label  \n",
       "0                     Mortgage  \n",
       "1                     Mortgage  \n",
       "2                     Mortgage  \n",
       "3  Credit card or prepaid card  \n",
       "4                Consumer Loan  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw=train_raw.rename(columns = {'consumer_complaint_narrative':'text', 'product':'label'})\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and segmentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing:\n",
    "The preprocessing step goes as follows:\n",
    "1. Remove all documents with fewer than 250 tokens. We want to concentrate only on long texts\n",
    "2. Consolidate the classes by combining those that are similar. (e.g.: \"Credit card\" or \"prepaid card\" complaints) :\n",
    " * Credit reporting‘ to ‘Credit reporting, credit repair services, or other personal consumerreports‘.\n",
    " * ‘Credit card‘ to ‘Credit card or prepaid card‘.\n",
    " * ‘Payday loan‘ to ‘Payday loan, title loan or personal loan‘1\n",
    " * ‘Virtual currency‘ to ‘Money transfer, virtual currency or money servic\n",
    "3. Remove all non-word characters\n",
    "4. Encode the labels\n",
    "5. Split the dataset in train set (80%) and validation set (20%).\n",
    "\n",
    "### 2. Segmentation and tokenization:\n",
    "First, each complaint is split into 200-token chunk with an overlap of 50 between each of them. This means that the last 50 tokens of a segment are the first 50 of the next segment.  \n",
    "Then, each segment is tokenized using BERT's tokenizer. This is needed for two main reasons:\n",
    "1. BERT's vocabulary is not made of just English words, but also subwords and single characters\n",
    "2. BERT does not take raw string as inputs. It needs:\n",
    "    - token ids: those values allow BERT to retrieve the tensor representation of a token\n",
    "    - input mask: a tensor of 0s and 1s that shows whether a token should be ignored (0) or not (1) by BERT\n",
    "    - segment ids: those are used to tell BERT what tokens form the first sentence and the second sentence (in the next sentence prediction task)  \n",
    "    \n",
    "The parameter `MAX_SEQ_LENGTH` ensures that any tokenized sentence longer that that number (200 in this case) will be truncated.  \n",
    "Each returned segment is given the same class as the document containing it.\n",
    "\n",
    "PyTorch offers the `Dataset` and `DataLoader` classes that make it easier to group the data reading and preparation operations while decreasing the memory usage in case the dataset is large.  \n",
    "We implemented the above steps in the `Custom_Dataset_Class.py` file. Our dataset class `ConsumerComplaintsDataset1` has a constructor (`__init__`) taking the necessary parameters to load the .csv file, segment the documents, and then tokenize them. It also preprocesses the data as explained in the preprocessing section.    \n",
    "The two other important methods are the following:\n",
    "- `__len__` returns the number of documents\n",
    "- `__getitem__` is the method where most of the work is done. It takes a tensor of idx values and returns the tokenized data:\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "As said above, there is differents approaches for overflowing tokens, we added the differents strategies (described in [the following paper](https://arxiv.org/abs/1905.05583)) via the use of the parameter `approach` in the initialisation of Consumer Complaints Dataset class, here is the differents value of the `approach` parameter to handle that situation :\n",
    "- **all**: overflowing tokens from a document are used to create new 200 token chunk with 50 tokens overlap between them\n",
    "- **head**: overflowing tokens are truncated. Only the first 200 tokens are used.\n",
    "- **tail**: only the last 200 tokens are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettoyage des données\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE=8\n",
    "EPOCH=3\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "#MAX_SIZE_DATASET=1000\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fine-tuning on the 200 tokens chunks:\n",
    "BERT is fine-tuned on the 200 tokens chunks\n",
    "\n",
    "In our implementation, we put a neural network on top of the pooled output from BERT, each BERT's input token has a embeding as an output, the embedding of the `CLS` token (the first token) corresponds to the pooled output of the all sentence input (the 200 tokens chunk in our case).\n",
    "\n",
    "The neural network is composed of a dense layer with a SoftMax activation function.\n",
    "\n",
    "This Model corresponds to the class `Bert_Classification_Model` defined in the file `Bert_Classification.py`. This class inherits from `torch.nn.Module` which is the parent of all neural network models.\n",
    "\n",
    "Then, in the function `train_loop_fun1` defined in `utils.py`, for each batch, the list of dictionaries containing the values for the token_ids, masks, token_type_ids, and targets are respectively concatenated into `torch.tensors` which are then fed into the model in order to get predictions and apply backpropagation according to the Cross Entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First segmetation approach: all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this approach, we will consider each chunk of 200 tokens as a new document, so if a document is split into 3 chunk of 200, tokens we will consider each chunks as a new document with the same label.\\\n",
    "We use this approach to fine tune BERT, so this model will be used as an input for RoBERT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/each_Chunk_as_Document.png](img/each_Chunk_as_Document.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== EPOCH 1 / 3 ===============\n",
      "\n",
      "___ batch index = 0 / 1715 (0.00%), loss = 2.1318, time = 0.28 secondes ___\n",
      "___ batch index = 250 / 1715 (14.58%), loss = 0.8838, time = 45.34 secondes ___\n",
      "___ batch index = 500 / 1715 (29.15%), loss = 0.7864, time = 44.90 secondes ___\n",
      "___ batch index = 750 / 1715 (43.73%), loss = 0.7725, time = 45.09 secondes ___\n",
      "___ batch index = 1000 / 1715 (58.31%), loss = 0.5372, time = 45.77 secondes ___\n",
      "___ batch index = 1250 / 1715 (72.89%), loss = 0.6988, time = 46.60 secondes ___\n",
      "___ batch index = 1500 / 1715 (87.46%), loss = 0.7093, time = 46.19 secondes ___\n",
      "\n",
      "*** avg_loss : 0.75, time : ~5.0 min (313.13 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 0.57, time : 37.34 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.8262952448545068, 'nb exemple': 11272, 'true_prediction': 9314, 'false_prediction': 1958}\n",
      "\t§§ model has been saved §§\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory model1 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jiarakul.s.aa/RoBERT_Recurrence_over_BERT/train.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bampere02/home/jiarakul.s.aa/RoBERT_Recurrence_over_BERT/train.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m batches_losses\u001b[39m.\u001b[39mappend(batches_losses_tmp)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bampere02/home/jiarakul.s.aa/RoBERT_Recurrence_over_BERT/train.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m§§ model has been saved §§\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bampere02/home/jiarakul.s.aa/RoBERT_Recurrence_over_BERT/train.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m torch\u001b[39m.\u001b[39;49msave(model, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodel1/model_epoch\u001b[39;49m\u001b[39m{\u001b[39;49;00mepoch\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)    \n",
      "File \u001b[0;32m~/miniconda3/envs/finarg/lib/python3.10/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finarg/lib/python3.10/site-packages/torch/serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[0;32m~/miniconda3/envs/finarg/lib/python3.10/site-packages/torch/serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory model1 does not exist."
     ]
    }
   ],
   "source": [
    "device=\"cuda:2\"\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "model=Bert_Classification_Model().to(device)\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=train_loop_fun1(train_data_loader, model, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=eval_loop_fun1(valid_data_loader, model, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(\"\\t§§ model has been saved §§\")\n",
    "    torch.save(model, f\"model1/model_epoch{epoch+1}.pt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.array([[np.mean(x) for x in batches_losses], [np.mean(x) for x in val_losses]]).T,\n",
    "                   columns=['Training', 'Validation']).plot(title=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.array(val_acc).T,\n",
    "                   columns=['Validation']).plot(title=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we will experience the [Truncation strategies presented in this paper](https://arxiv.org/abs/1905.05583)\n",
    "\n",
    "such as:  \n",
    "* Tunction Method:  \n",
    "    + Head only  \n",
    "    + Tail only  \n",
    "* Hierarchical Method:  \n",
    "    + Mean pooling  \n",
    "    + Max pooling  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "The Truncation methods applies to the input of the BERT model (the Tokens)\n",
    " \n",
    "Usually, the key information of an article is at the beginning and end. We\n",
    "use two different methods of truncate text to perform BERT fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second segmentation approach: head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this approach, we will keep only the first chunk of 200 tokens for each documents, so if a document is split into 3 chunk of 200, we will only keep the first chunk and we will not keep the last two chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/Head_Truncation.png](img/Head_Truncation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE=8\n",
    "EPOCH=1\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "#MAX_SIZE_DATASET=1000\n",
    "\n",
    "approach='head'\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN,\n",
    "    approach=approach)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "device=\"cpu\"\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "model=BertConsumerComplaints().to(device)\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=train_loop_fun1(train_data_loader, model, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=eval_loop_fun1(valid_data_loader, model, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(f\"\\t§§ Truncation {approach} only model has been saved §§\")\n",
    "    torch.save(model, f\"model_truncation/{approach}_only/model_{approach}_only_epoch{epoch+1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third segmetation approach: tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this approach, we will keep only the last chunk of 200 tokens for each documents, so if a document is split into 3 chunk of 200, we will only keep the last chunk and we will not keep the first two chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/Tail_Truncation.png](img/Tail_Truncation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE=8\n",
    "EPOCH=1\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "#MAX_SIZE_DATASET=1000\n",
    "approach='tail'\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN,\n",
    "    approach=approach)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "device=\"cpu\"\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "model=BertConsumerComplaints().to(device)\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=train_loop_fun1(train_data_loader, model, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=eval_loop_fun1(valid_data_loader, model, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(f\"\\t§§ Truncation {approach} only model has been saved §§\")\n",
    "    torch.save(model, f\"model_truncation/{approach}_only/model_{approach}_only_epoch{epoch+1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Hierarchical methods applies to the ouputs of the Bert model (the embbeding)\n",
    "The input text is firstly divided into k = L/510 fractions, which is fed into BERT to obtain the representation of the k text fractions. The representation of each fraction is the hidden state of the `[CLS]` tokens of the last layer. Then we use mean pooling, max pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Pooling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this approach, we average the embedding of all k chunks across each dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/Mean_Pooling_Hierarchical.png](img/Mean_Pooling_Hierarchical.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE=3\n",
    "EPOCH=1\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "#MAX_SIZE_DATASET=1000\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "device=\"cpu\"\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "pooling_method=\"mean\"\n",
    "model_hierarchical=BERT_Hierarchical_Model(pooling_method=pooling_method).to(device)\n",
    "optimizer=AdamW(model_hierarchical.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=rnn_train_loop_fun1(train_data_loader, model_hierarchical, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=rnn_eval_loop_fun1(valid_data_loader, model_hierarchical, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")    \n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(f\"\\t§§ the Hierarchical {pooling_method} pooling model has been saved §§\")\n",
    "    torch.save(model_hierarchical, f\"model_hierarchical/{pooling_method}_pooling/model_{pooling_method}_pooling_epoch{epoch+1}.pt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Pooling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this approach, we take the maximum embedding of all the k chunks across each dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/Max_Pooling_Hierarchical.png](img/Max_Pooling_Hierarchical.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE=3\n",
    "EPOCH=1\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "#MAX_SIZE_DATASET=1000\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "device=\"cpu\"\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "pooling_method=\"max\"\n",
    "model_hierarchical=BERT_Hierarchical_Model(pooling_method=pooling_method).to(device)\n",
    "optimizer=AdamW(model_hierarchical.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=rnn_train_loop_fun1(train_data_loader, model_hierarchical, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=rnn_eval_loop_fun1(valid_data_loader, model_hierarchical, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")    \n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(f\"\\t§§ the Hierarchical {pooling_method} pooling model has been saved §§\")\n",
    "    torch.save(model_hierarchical, f\"model_hierarchical/{pooling_method}_pooling/model_{pooling_method}_pooling_epoch{epoch+1}.pt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERT RNN classifier on top of the Fine Tuned Bert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The input text is firstly divided into k = L/510 fractions, which is fed into the fine tuned BERT (as describe above, *cf: First segmetation approach: all*) to obtain the representation of the k text chunks. The representation of each fraction is the hidden state of the `[CLS]` tokens of the last layer.\n",
    " \n",
    "Each chunk embedding (representation) become the input of an LSTM cell, this way the order is preserved and the length of the document is not a limitation anymore because of the dynamic aspect of the LSTM that allow different variable sequence lengths (accros different batches)\n",
    "\n",
    "we then pass the last hidden state (nbDoc * 100) to a neural network with the same architecture (as describe above, we use the same neural network architecture for classification through the all notebook, [cf: 3. Fine-tuning on the 200 tokens chunks](#3.-Fine-tuning-on-the-200-tokens-chunks:)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/RoBERT.png](img/RoBERT.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE=3\n",
    "EPOCH=3\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "#MAX_SIZE_DATASET=1000\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "device=\"cpu\"\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "model=torch.load(\"model1/model_epoch2.pt\")\n",
    "\n",
    "model_rnn=RoBERT_Model(bertFineTuned=list(model.children())[0]).to(device)\n",
    "optimizer=AdamW(model_rnn.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=rnn_train_loop_fun1(train_data_loader, model_rnn, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=rnn_eval_loop_fun1(valid_data_loader, model_rnn, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")    \n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(\"\\t§§ the RNN model has been saved §§\")\n",
    "    torch.save(model_rnn, f\"model_rnn1/model_rnn_epoch{epoch+1}.pt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.array([[np.mean(x) for x in batches_losses], [np.mean(x) for x in val_losses]]).T,\n",
    "                   columns=['Training', 'Validation']).plot(title=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.array(val_acc).T,\n",
    "                   columns=['Validation']).plot(title=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=pd.DataFrame({\"all\":[0.74, 0.57, 0.83], \"head only\":[0.62, 0.45, 0.87], \"tail only\":[0.93, 0.75, 0.77], \"mean pooling\":[0.62, 0.47, 0.87], \"max pooling\":[0.65, 0.45, 0.87], \"RoBERT\":[0.46, 0.34, 0.91]}, index=[\"avg_loss_train\", \"avg_loss_val\", \"accuracy\"])\n",
    "summary.columns.name = 'after one epoch'\n",
    "summary.style.set_properties(\n",
    "    subset=['RoBERT'], \n",
    "    **{'font-weight': 'bold'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the RoBERT Model give the best result, so we can conclude that this Model a net State Of the Art improvement in term of the loss function and the accuracy, the second model is the head only, this make sense because we can imagine that the consumer introduce his complain within the first part of his comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
